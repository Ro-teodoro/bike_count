{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Building Footprint Extraction Using Satellite RGB and LiDAR Elevation\n",
    "\n",
    "In this notebook, we will show how to utilize both SpaceNet satellite image and USGS 3DEP LiDAR data (elevation attribute) to automatically extract building footprints. This task was used in three previous SpaceNet challenges: 1st, 2nd, and 4th."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.p3.8xlarge notebook instance._\n",
    "\n",
    "First of all, if you haven't done so, please follow instructions in `README.md` to run `setup-env.sh` and `download-from-s3.sh` scripts to properly set up the Conda environment and download necessary files from S3 buckets prepared for this tutorial. Before proceeding, make sure this notebook connects with the proper kernel (`conda_[name]`, `[name]` is name of the new Conda environment you just created)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json, random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from p_tqdm import p_umap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "import torch                 \n",
    "from torch import nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External modules\n",
    "\n",
    "We made some customized modification to the external module `solaris` in `libs/` directory. Please refer to their [GitHub page](https://github.com/CosmiQ/solaris) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libs.solaris as sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIG01\\AppData\\Local\\Temp\\ipykernel_18768\\3301049148.py:5: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-notebook')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The datasets used in this notebook are saved in directory `./data/buildings/`.\n",
    "\n",
    "1. **SpaceNet satellite images**:\n",
    "There are 3850 tiles (\"chips\") of 200m$\\times$200m satellite images/labels in this training dataset (`SN2/AOI_2_Vegas`). See [link](https://spacenet.ai/las-vegas/) for more information. We extract the ~0.3m resolution pan-sharpened RGB (`PS-RGB`) 3-channel satellite images and perform white balancing. On the other hand, the `geojson_buildings/` directory contains GeoJSON files for building ground truths.\n",
    "\n",
    "2. **USGS 3DEP LiDAR**:\n",
    "This is [a large USGS project](https://www.usgs.gov/core-science-systems/ngp/3dep) that covers many regions throughout US. Also see [here](https://usgs.entwine.io/) for cool visualizations. We extract 3D point clouds in Las Vegas region that overlap with the SpaceNet data, project them into corresponding 2D tiles (3084 in total). The LiDAR data have two attributes: elevation and reflection intensity. For building extraction, we will use the elevation attribute after normalizing the elevation values.\n",
    "\n",
    "At the end, we merge the LiDAR elevation attribute as an additional channel to the RGB image, and save these 4-channel images to `RGB+ELEV/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './download/data/buildings/'\n",
    "img_dir = os.path.join(data_dir, 'RGB+ELEV')\n",
    "bldg_dir = os.path.join(data_dir, 'geojson_buildings')\n",
    "\n",
    "# Prefix of all filename - naming convention\n",
    "prefix = 'SN2_buildings_train_AOI_2_Vegas_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a data sample\n",
    "sample = 'img1423' # chip ID, img? format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in 4-channel image from GeoTIFF.\n",
    "img_file = prefix + 'RGB+ELEV_' + sample + '.tif'\n",
    "img_path = os.path.join(img_dir, img_file)\n",
    "img = skimage.io.imread(img_path)\n",
    "rgb = img[..., :3]\n",
    "elev = img[..., -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in GeoJSON file and convert polygons to footprint mask.\n",
    "bldg_file = prefix + 'geojson_buildings_' + sample + '.geojson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bldg_path = os.path.join(bldg_dir, bldg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "mask = sol.vector.mask.footprint_mask(bldg_path, reference_im=img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display satellite image and building footprint mask.\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax[0].imshow(rgb)\n",
    "ax[0].set_title('Satellite image')\n",
    "ax[1].imshow(elev, cmap='gray', vmin=0, vmax=5000)\n",
    "ax[1].set_title('LiDAR elevation')\n",
    "ax[2].imshow(mask, cmap='Blues')\n",
    "ax[2].set_title('Building footprint masks')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network model\n",
    "\n",
    "The SpaceNet images come with GeoJSON annotations as polygon vectors. These cannot be directly accepted by neural network; instead, we should convert them to binary mask images (as shown above). These masks will be the \"ground truth labels\" to train our segmentation network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dir = os.path.join(data_dir, 'mask_buildings')\n",
    "\n",
    "# This step is time-consuming, and only needs to be run once.\n",
    "# After you have all the mask labels ready in folder `mask_buildings/`,\n",
    "# this operation will be skipped.\n",
    "if not os.path.exists(mask_dir):\n",
    "    os.mkdir(mask_dir)\n",
    "    \n",
    "    img_file_list = [f for f in os.listdir(img_dir) if f.endswith('.tif')]\n",
    "    for img_file in tqdm(img_file_list):\n",
    "        # Get the `img[number]` chip ID from file name.\n",
    "        chip_id = os.path.splitext(img_file)[0].split('_')[-1]\n",
    "        bldg_file = prefix + 'geojson_buildings_' + chip_id + '.geojson'\n",
    "        mask_file = prefix + 'mask_buildings_' + chip_id + '.tif'\n",
    "        \n",
    "        _ = sol.vector.mask.footprint_mask(\n",
    "            os.path.join(bldg_dir, bldg_file),\n",
    "            out_file=os.path.join(mask_dir, mask_file),\n",
    "            reference_im=os.path.join(img_dir, img_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and test data\n",
    "\n",
    "SpaceNet provides two groups of data: a `training` directory to train models and `test_public` directory for final inference, scoring, and ranking. Because the ground truth files for `test_public` data are not publicly available, we won't be able to reproduce exactly the winners' score.\n",
    "\n",
    "On the other hand, we can split a part of `training` data, hold this part of data out as a simulated blind test. The filenames for each data group are saved to CSV file `split_train_data.csv` and `split_blind_test.csv`, respectively. We can split the 3084 image tiles by 30%/70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lists of training image/label files.\n",
    "img_file_list = [f for f in os.listdir(img_dir) if f.endswith('.tif')]\n",
    "mask_file_list = [f for f in os.listdir(mask_dir) if f.endswith('.tif')]\n",
    "# Extract the mask files that have corresponding RGB+LIDAR images.\n",
    "mask_file_subset = [f for f in mask_file_list\n",
    "    if f.replace('mask_buildings_', 'RGB+ELEV_') in img_file_list]\n",
    "img_path_list = [os.path.join(img_dir, f) for f in img_file_list]\n",
    "mask_path_list = [os.path.join(mask_dir, f) for f in mask_file_subset]\n",
    "assert len(img_path_list) == len(mask_path_list)\n",
    "# Sort the list by filenames.\n",
    "img_path_list.sort()\n",
    "mask_path_list.sort()\n",
    "\n",
    "# Create Pandas data frame, containing columns 'image' and 'label'.\n",
    "total_df = pd.DataFrame({'image': img_path_list,\n",
    "                         'label': mask_path_list})\n",
    "# Split this data frame to training data and blind test data.\n",
    "split_mask = np.random.rand(len(total_df)) < 0.7\n",
    "train_df = total_df[split_mask]\n",
    "test_df = total_df[~split_mask]\n",
    "\n",
    "# Save the data frames to CSV files.\n",
    "train_csv_path = './data/buildings/split_train_data.csv'\n",
    "test_csv_path = './data/buildings/split_blind_test.csv'\n",
    "# I have put the CSV files in the notebook directory already; the following;\n",
    "# CSV output lines are commented out for the sake of consistency.\n",
    "# If you want to generate your own train/test set, uncomment them, run again.\n",
    "# train_df.to_csv(train_csv_path)\n",
    "# test_df.to_csv(test_csv_path)\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "print('{} images in total, {} - train, {} - test.'.format(\n",
    "    len(total_df), len(train_df), len(test_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training setup\n",
    "\n",
    "Now that train/test data are ready to go, we will set up the network model training. The library `solaris` helps building a ML pipeline for overhead imagery, which handles data loading, pre-processing, augmentation, model training/inference, and performance evaluation.\n",
    "\n",
    "For this notebook, all training setups in `.yml` format are saved in directory `configs/buildings/`:\n",
    "1. `RGB-only.yml`: network only uses RGB images as input (3-channel);\n",
    "2. `ELEV-only.yml`: network only uses LiDAR elevation images as input (1-channel);\n",
    "3. `RGB+ELEV.yml`: network uses both RGB and LiDAR elevation as merged input (4-channel).\n",
    "\n",
    "In the following cell, please select the config file you would like to experiment. After completing this notebook, feel free to come back here, change to other config or create your own setup to see what's different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------#\n",
    "# Select the config file you want to use\n",
    "config = sol.utils.config.parse('./configs/buildings/RGB+ELEV.yml')\n",
    "# --------------------------------------------#\n",
    "\n",
    "# Link training datasets to config.\n",
    "config['training_data_csv'] = train_csv_path\n",
    "config['inference_data_csv'] = test_csv_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A side note: since we do not explicitly assign `validation_data_csv` in the config, `solaris` will automatically split a random portion (specified by `val_holdout_frac`, in this case 0.05) of training data for validation during training.\n",
    "\n",
    "Next, let's load a customized VGG16-Unet from `networks/` directory. This algorithm is from the winner `XD_XD` from SpaceNet challenge 4 ([link to repo](https://github.com/SpaceNetChallenge/SpaceNet_Off_Nadir_Solutions/tree/master/XD_XD)). We modified the code so that it takes multi-channel input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load customized multi-channel input VGG16-Unet model.\n",
    "from networks.vgg16_unet import get_modified_vgg16_unet\n",
    "\n",
    "custom_model = get_modified_vgg16_unet(\n",
    "    in_channels=config['data_specs']['channels'])\n",
    "custom_model_dict = {\n",
    "    'model_name': 'modified_vgg16_unet',\n",
    "    'weight_path': None,\n",
    "    'weight_url': None,\n",
    "    'arch': custom_model\n",
    "}\n",
    "\n",
    "# Create trainer\n",
    "trainer = sol.nets.train.Trainer(config, custom_model_dict=custom_model_dict)\n",
    "# Show model architecture\n",
    "# print(trainer.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set for training!\n",
    "\n",
    "Since training takes a long time, we also provide our trained model weights in `models/buildings/` directory. The next inference section will use these trained weights. However, if your time and resource allows, uncomment and run the following cell to train your own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------#\n",
    "# This takes ~1.5 hours to finish (100 epochs).\n",
    "# Training is skipped by default.\n",
    "# Uncomment the following line if you want to repeat the trainig process.\n",
    "# trainer.train()\n",
    "# --------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on sample data\n",
    "\n",
    "After training, we can test the model by first trying on the sample data displayed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model_dict = {\n",
    "    'model_name': 'modified_vgg16_unet',\n",
    "    'weight_path': config['training']['model_dest_path'],\n",
    "    'weight_url': None,\n",
    "    'arch': custom_model}\n",
    "config['train'] = False\n",
    "inferer = sol.nets.infer.Inferer(config, custom_model_dict=custom_model_dict)\n",
    "\n",
    "sample_df = pd.DataFrame({'image': [img_path]}) # previous sample image\n",
    "inferer(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prediction result from the output directory.\n",
    "pred_dir = config['inference']['output_dir']\n",
    "pred_file = prefix + 'RGB+ELEV_' + sample + '.tif'\n",
    "pred_path = os.path.join(pred_dir, pred_file)\n",
    "pred = skimage.io.imread(pred_path)[..., 0]\n",
    "\n",
    "# Display satellite image, LiDAR, prediction mask, and ground truth mask.\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "ax[0][0].imshow(rgb)\n",
    "ax[0][0].set_title('Satellite image')\n",
    "ax[0][1].imshow(elev, cmap='gray', vmin=0, vmax=5000)\n",
    "ax[0][1].set_title('LiDAR elevation')\n",
    "ax[1][0].imshow(pred>0, cmap='Blues')\n",
    "ax[1][0].set_title('Predicted footprint')\n",
    "ax[1][1].imshow(mask, cmap='Blues')\n",
    "ax[1][1].set_title('Ground truth footprint')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result looks good. Meanwhile, other than visual comparison, a quantitative metric can help us better understand the model's performance. SpaceNet building extraction challenge evaluates models by F-1 scores for IoU>=50%. Now let's evaluate the F-1 score of the result above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert GeoJSON polygons to binary masks for training and visualization;\n",
    "# similarly, now we will do the reverse: convert prediction masks\n",
    "# to polygon GeoJSON as \"proposal\".\n",
    "results_dir = Path(config['inference']['output_dir']).parent\n",
    "prop_dir = os.path.join(results_dir, 'prop_geojson')\n",
    "os.makedirs(prop_dir, exist_ok=True)\n",
    "prop_file = bldg_file # same filename as the ground truth GeoJSON\n",
    "prop_path = os.path.join(prop_dir, prop_file)\n",
    "prop = sol.vector.mask.mask_to_poly_geojson(\n",
    "    pred_arr=pred,\n",
    "    reference_im=img_path,\n",
    "    do_transform=True,\n",
    "    min_area=1e-10,\n",
    "    output_path=prop_path\n",
    ") # save to geojson file in the output folder\n",
    "\n",
    "# Create evaluator and load the ground truth.\n",
    "evaluator = sol.eval.base.Evaluator(bldg_path)\n",
    "\n",
    "# Evaluate F-1 score by IoU>=50% detections.\n",
    "evaluator.load_proposal(prop_path, conf_field_list=[]) # only one class\n",
    "score = evaluator.eval_iou(miniou=0.5, calculate_class_scores=False)\n",
    "score_df = pd.DataFrame.from_records(score)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch inference and scoring on all test data\n",
    "\n",
    "Next, we will do inference on all test data in `test_df`. To compute a collective F-1 score, we will use the aggregation of true positive, false positive, and false negative for all test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch inference on all test data. This may take a while.\n",
    "inferer(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of prediction masks and reference images.\n",
    "pred_file_list = [f for f in os.listdir(pred_dir) if f.endswith('.tif')]\n",
    "pred_file_list.sort()\n",
    "img_path_list = list(test_df['image'])\n",
    "\n",
    "# Convert these probability maps to building polygons.\n",
    "def pred_to_prop(pred_file, img_path):\n",
    "    pred_path = os.path.join(pred_dir, pred_file)\n",
    "    pred = skimage.io.imread(pred_path)[..., 0]\n",
    "    prop_file = \\\n",
    "        pred_file.replace('RGB+ELEV', 'geojson_buildings').replace('tif', 'geojson')\n",
    "    prop_path = os.path.join(prop_dir, prop_file)\n",
    "    prop = sol.vector.mask.mask_to_poly_geojson(\n",
    "        pred_arr=pred,\n",
    "        reference_im=img_path,\n",
    "        do_transform=True,\n",
    "        min_area=1e-10,\n",
    "        output_path=prop_path\n",
    "    )\n",
    "\n",
    "# Using p_umap (paralle unordered map) to accelerate\n",
    "_ = p_umap(pred_to_prop, pred_file_list, img_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of proposal polygons and ground truth polygons.\n",
    "prop_file_list = [f for f in os.listdir(prop_dir) if f.endswith('.geojson')]\n",
    "prop_file_list.sort()\n",
    "prop_path_list = [os.path.join(prop_dir, f) for f in prop_file_list]\n",
    "bldg_file_list = prop_file_list # they share same filenames\n",
    "bldg_path_list = [os.path.join(bldg_dir, f) for f in bldg_file_list]\n",
    "\n",
    "# Evaluate scores.\n",
    "def compute_score(prop_path, bldg_path):\n",
    "    evaluator = sol.eval.base.Evaluator(bldg_path)\n",
    "    evaluator.load_proposal(prop_path, conf_field_list=[])\n",
    "    score = evaluator.eval_iou(miniou=0.5, calculate_class_scores=False)\n",
    "    # score_list.append(score[0]) # only have one class\n",
    "    return score[0] # only have one class\n",
    "\n",
    "# Using p_umap (paralle unordered map) to accelerate\n",
    "score_list = p_umap(compute_score, prop_path_list, bldg_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame.from_records(score_list)\n",
    "\n",
    "# Compute aggregated Precision, Recall, and F-1 score.\n",
    "tp_agg = score_df['TruePos'].sum()\n",
    "fp_agg = score_df['FalsePos'].sum()\n",
    "fn_agg = score_df['FalseNeg'].sum()\n",
    "Precision = tp_agg / (tp_agg + fp_agg)\n",
    "Recall = tp_agg / (tp_agg + fn_agg)\n",
    "F1 = 2 * Precision * Recall / (Precision + Recall)\n",
    "print(f'Precision = {Precision}')\n",
    "print(f'Recall    = {Recall}')\n",
    "print(f'F-1 Score = {F1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad at all! For reference, here are the [scores of the winners](https://medium.com/the-downlinq/2nd-spacenet-competition-winners-code-release-c7473eea7c11) (scores for Las Vegas data) from the original contest:\n",
    "\n",
    "| Rank  \t| Name   \t| F-1 Score \t|\n",
    "|-------\t|--------\t|------------\t|\n",
    "| 1     \t| XD_XD  \t| 0.885       \t|\n",
    "| 2     \t| wleite \t| 0.829        \t|\n",
    "| 3     \t| nofto  \t| 0.787        \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results\n",
    "\n",
    "In this section, we visually check some results. The next cell randomly sample from the result list. Rerun as many times as you wish to see different figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly check the network predictions. Feel free to rerun this cell!\n",
    "file = random.choice(pred_file_list)\n",
    "# Or you can select the 'img1423.tif' we saw before.\n",
    "# file = 'SN2_buildings_train_AOI_2_Vegas_RGB+ELEV_img1423.tif'\n",
    "img_path = os.path.join(img_dir, file)\n",
    "pred_path = os.path.join(pred_dir, file)\n",
    "mask_path = os.path.join(mask_dir, file.replace('RGB+ELEV', 'mask_buildings'))\n",
    "\n",
    "img = skimage.io.imread(img_path)\n",
    "rgb = img[..., :3]\n",
    "elev = img[..., -1]\n",
    "pred = skimage.io.imread(pred_path)[..., 0]\n",
    "mask = skimage.io.imread(mask_path)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "ax[0][0].imshow(rgb)\n",
    "ax[0][0].set_title('RGB - {}'.format(file.split('_')[-1]))\n",
    "ax[0][1].imshow(elev, cmap='gray', vmin=0, vmax=5000)\n",
    "ax[0][1].set_title('LIDAR Elevation')\n",
    "ax[1][0].imshow(pred>0, cmap='Blues')\n",
    "ax[1][0].set_title('Prediction')\n",
    "ax[1][1].imshow(mask, cmap='Blues')\n",
    "ax[1][1].set_title('Ground Truth')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper figure\n",
    "\n",
    "The following cell generates figure used in the tutorial proposal paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_sample_list = random.choices(pred_file_list, k=3)\n",
    "img_sample_list = [\n",
    "    'SN2_buildings_train_AOI_2_Vegas_RGB+ELEV_img3352.tif',\n",
    "    'SN2_buildings_train_AOI_2_Vegas_RGB+ELEV_img5284.tif',\n",
    "    'SN2_buildings_train_AOI_2_Vegas_RGB+ELEV_img3414.tif',\n",
    "]\n",
    "print(img_sample_list)\n",
    "\n",
    "fig, ax = plt.subplots(3, 4, figsize=(12, 9))\n",
    "plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0.05, hspace=0.05)\n",
    "\n",
    "for i, img_file in enumerate(img_sample_list):\n",
    "    img_path = os.path.join(img_dir, img_file)\n",
    "    img = skimage.io.imread(img_path)\n",
    "    rgb = img[..., :3] # select RGB channels\n",
    "    elev = img[..., -1]\n",
    "    \n",
    "    pred_path = os.path.join(pred_dir, img_file)\n",
    "    pred = skimage.io.imread(pred_path)[..., 0]\n",
    "    \n",
    "    mask_file = img_file.replace('RGB+ELEV', 'mask_buildings')\n",
    "    mask_path = os.path.join(mask_dir, mask_file)\n",
    "    mask = skimage.io.imread(mask_path)\n",
    "    \n",
    "    row = ax[i]\n",
    "    row[0].imshow(rgb)\n",
    "    row[0].axis('off')\n",
    "    row[1].imshow(elev, cmap='gray', vmin=0, vmax=5000)\n",
    "    row[1].axis('off')\n",
    "    row[2].imshow(pred>0, alpha=(pred>0).astype('float'), cmap='Blues')\n",
    "    plt.setp(row[2].get_yticklabels(), visible=False)\n",
    "    plt.setp(row[2].get_xticklabels(), visible=False)\n",
    "    row[2].tick_params(axis='both', which='both', length=0)\n",
    "    row[3].imshow(mask, alpha=(mask>0).astype('float'), cmap='Blues')\n",
    "    plt.setp(row[3].get_yticklabels(), visible=False)\n",
    "    plt.setp(row[3].get_xticklabels(), visible=False)\n",
    "    row[3].tick_params(axis='both', which='both', length=0)\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training logs are available in `data/buildings/train_logs/` directory for the three setups. Here are a comparison of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log2data(logfile_path, smooth_n=5):\n",
    "    f = open(logfile_path, 'r')\n",
    "    content = f.read().split('\\n')\n",
    "    val_loss = [float(l.split(' ')[-1]) for l in content[13::15]]\n",
    "    val_loss = np.convolve(val_loss, np.ones((smooth_n,))/smooth_n, mode='valid')\n",
    "    return val_loss\n",
    "\n",
    "both_loss = log2data('./data/buildings/train_logs/RGB+ELEV_train_log.txt')\n",
    "rgb_loss = log2data('./data/buildings/train_logs/RGB-only_train_log.txt')\n",
    "elev_loss = log2data('./data/buildings/train_logs/ELEV-only_train_log.txt')\n",
    "\n",
    "plt.plot(elev_loss, label='LiDAR elevation only')\n",
    "plt.plot(rgb_loss, label='RGB only')\n",
    "plt.plot(both_loss, label='RGB + LiDAR elevation')\n",
    "plt.ylabel('Loss value')\n",
    "plt.ylim(1.2, 4)\n",
    "plt.xlabel('Training epoch')\n",
    "plt.xlim(0, 100)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This concludes the building extraction notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AWS-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc-showmarkdowntxt": false,
  "vscode": {
   "interpreter": {
    "hash": "ab17229d7ef8a2352d74e9197be096b048eb467d2bea9d39c6e4962fb2706752"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
